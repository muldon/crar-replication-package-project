Query: Python: how to get the final output of multiple system commands?
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/14971429)
 Just pass the  shell=True  option to subprocess 

  import subprocess
subprocess.check_output('ps -ef | grep something | wc -l', shell=True)
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/14971409)
 Well, another alternative would just be to implement part of the command in plain Python. For example, 

  count = 0
for line in subprocess.check_output(['ps', '-ef']).split('\n'):
    if something in line: # or re.search(something, line) to use regex
        count += 1
print count
  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/14971443)
 For a no-shell, clean version using the subprocess module, you can use the following example (http://docs.python.org/2/library/subprocess.html#replacing-shell-pipeline): 

  output = `dmesg | grep hda`
  

  

  p1 = Popen(["dmesg"], stdout=PIPE)
p2 = Popen(["grep", "hda"], stdin=p1.stdout, stdout=PIPE)
p1.stdout.close()  # Allow p1 to receive a SIGPIPE if p2 exits.
output = p2.communicate()[0]
  

 The Python program essentially does here what the shell does: it sends the output of each command to the next one in turn. An advantage of this approach is that the programmer has full control on the individual standard error outputs of the commands (they can be suppressed if needed, logged, etc.). 

 That said, I generally prefer to use instead the  subprocess.check_output('ps -ef | grep something | wc -l', shell=True)  shell-delegation approach suggested by nneonneo: it is general, very legible and convenient. 


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/39215468)
 Join commands with "&&". 

  os.system('echo a > outputa.txt && echo b > outputb.txt')
  


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/92395)
 Here's a summary of the ways to call external programs and the advantages and disadvantages of each: 

 
   os.system("some_command with args")  passes the command and arguments to your system's shell.  This is nice because you can actually run multiple commands at once in this manner and set up pipes and input/output redirection.  For example:   

  os.system("some_command < input_file | another_command > output_file")  
  

 However, while this is convenient, you have to manually handle the escaping of shell characters such as spaces, etc.  On the other hand, this also lets you run commands which are simply shell commands and not actually external programs.  See https://docs.python.org/2/library/os.html#os.system.  
   stream = os.popen("some_command with args")  will do the same thing as  os.system  except that it gives you a file-like object that you can use to access standard input/output for that process.  There are 3 other variants of popen that all handle the i/o slightly differently.  If you pass everything as a string, then your command is passed to the shell; if you pass them as a list then you don't need to worry about escaping anything.  See <a href="https://docs.python.org/2/library/os.html#os.popen" .  
  The  Popen  class of the  subprocess  module.  This is intended as a replacement for  os.popen  but has the downside of being slightly more complicated by virtue of being so comprehensive.  For example, you'd say: 

  print subprocess.Popen("echo Hello World", shell=True, stdout=subprocess.PIPE).stdout.read()
  

   

  print os.popen("echo Hello World").read()
  

 but it is nice to have all of the options there in one unified class instead of 4 different popen functions.  See <a href="https://docs.python.org/2/library/subprocess.html#popen-constructor" .  
  The  call  function from the  subprocess  module.  This is basically just like the  Popen  class and takes all of the same arguments, but it simply waits until the command completes and gives you the return code.  For example: 

  return_code = subprocess.call("echo Hello World", shell=True)  
  

 See <a href="https://docs.python.org/2/library/subprocess.html#subprocess.call" .  
  If you're on Python 3.5 or later, you can use the new https://docs.python.org/3.5/library/subprocess.html#subprocess.run function, which is a lot like the above but even more flexible and returns a https://docs.python.org/3.5/library/subprocess.html#subprocess.CompletedProcess object when the command finishes executing.  
  The os module also has all of the fork/exec/spawn functions that you'd have in a C program, but I don't recommend using them directly.  
 

 The  subprocess  module should probably be what you use. 

 Finally please be aware that for all methods where you pass the final command to be executed by the shell as a string and you are responsible for escaping it.  There are serious security implications  if any part of the string that you pass can not be fully trusted. For example, if a user is entering some/any part of the string. If you are unsure, only use these methods with constants. To give you a hint of the implications consider this code: 

  print subprocess.Popen("echo %s " % user_input, stdout=PIPE).stdout.read()
  

 and imagine that the user enters "my mama didnt love me && rm -rf /". 


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/44836024)
 You separate the lines with & 

  os.system("ls -l & <some command> & launchMyApp")
  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/40319875)
 There are lots of different libraries which allow you to call external commands with Python. For each library I've given a description and shown an example of calling an external command. The command I used as the example is  ls -l  (list all files). If you want to find out more about any of the libraries I've listed and linked the documentation for each of them. 

 
    Sources:  
 

 
 subprocess: https://docs.python.org/3.5/library/subprocess.html 
 shlex: https://docs.python.org/3/library/shlex.html 
 os: https://docs.python.org/3.5/library/os.html 
 sh: https://amoffat.github.io/sh/ 
 plumbum: https://plumbum.readthedocs.io/en/latest/ 
 pexpect: https://pexpect.readthedocs.io/en/stable/ 
 fabric: http://www.fabfile.org/ 
 envoy: https://github.com/kennethreitz/envoy 
 commands: https://docs.python.org/2/library/commands.html 
 

 
   
     
        These are all the libraries:  
     
   
 

 Hopefully this will help you make a decision on which library to use :) 

 
    subprocess  
 

 Subprocess allows you to call external commands and connect them to their input/output/error pipes (stdin, stdout, and stderr). Subprocess is the default choice for running commands, but sometimes other modules are better. 

  subprocess.run(["ls", "-l"]) # Run command
subprocess.run(["ls", "-l"], stdout=subprocess.PIPE) # This will run the command and return any output
subprocess.run(shlex.split("ls -l")) # You can also use the shlex library to split the command
  

 
    os  
 

 os is used for "operating system dependent functionality". It can also be used to call external commands with  os.system  and  os.popen  (Note: There is also a subprocess.popen). os will always run the shell and is a simple alternative for people who don't need to, or don't know how to use  subprocess.run . 

  os.system("ls -l") # run command
os.popen("ls -l").read() # This will run the command and return any output
  

 
    sh  
 

 sh is a subprocess interface which lets you call programs as if they were functions. This is useful if you want to run a command multiple times. 

  sh.ls("-l") # Run command normally
ls_cmd = sh.Command("ls") # Save command as a variable
ls_cmd() # Run command as if it were a function
  

 
    plumbum  
 

 plumbum is a library for "script-like" Python programs. You can call programs like functions as in  sh . Plumbum is useful if you want to run a pipeline without the shell. 

  ls_cmd = plumbum.local("ls -l") # get command
ls_cmd() # run command
  

 
    pexpect  
 

 pexpect lets you spawn child applications, control them and find patterns in their output. This is a better alternative to subprocess for commands that expect a tty on Unix. 

  pexpect.run("ls -l") # Run command as normal
child = pexpect.spawn('scp foo user@example.com:.') # Spawns child application
child.expect('Password:') # When this is the output
child.sendline('mypassword')
  

 
    fabric  
 

 fabric is a Python 2.5 and 2.7 library. It allows you to execute local and remote shell commands. Fabric is simple alternative for running commands in a secure shell (SSH) 

  fabric.operations.local('ls -l') # Run command as normal
fabric.operations.local('ls -l', capture = True) # Run command and receive output
  

 
    envoy  
 

 envoy is known as "subprocess for humans". It is used as a convenience wrapper around the  subprocess  module. 

  r = envoy.run("ls -l") # Run command
r.std_out # get output
  

 
    commands  
 

  commands  contains wrapper functions for  os.popen , but it has been removed from Python 3 since  subprocess  is a better alternative. 

 The edit was based on J.F. Sebastian's comment. 


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/18330157)
 I assume that  cat ,  grep  are just example commands otherwise you could use a pure Python solution without subprocesses e.g.: 

  for line in simple.splitlines():
    if "line" in line:
       print(line)
  

 Or if you want to use  grep : 

  from subprocess import Popen, PIPE

output = Popen(['grep', 'line'], stdin=PIPE, stdout=PIPE).communicate(simple)[0]
print output,
  

 You can pass the output of the first command to the second one without storing it in a string first: 

  from subprocess import Popen, PIPE
from threading import Thread

# start commands in parallel
first = Popen(first_command, stdin=PIPE, stdout=PIPE)
second = Popen(second_command, stdin=first.stdout, stdout=PIPE)
first.stdout.close() # notify `first` if `second` exits 
first.stdout = None # avoid I/O on it in `.communicate()`

# feed input to the first command
Thread(target=first.communicate, args=[simple]).start() # avoid blocking

# get output from the second command at the same time
output = second.communicate()[0]
print output,
  

 If you don't want to store all input/output in memory; you might need threads (to read/write in chunks without blocking) or a select loop (works on POSIX). 

 If there are multiple commands, it might be more readable just to use the shell directly as suggested by https://stackoverflow.com/a/18326151/4279 or use https://stackoverflow.com/a/16709666/4279. 


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/9616506)
 What you want here is (apparently) a series of pipes: 

  com[0] | com[1] | ... | com[n-1]
  

 The simplest method, if you don't have to worry about "bad" characters and shells, is to just join them all up into one big shell command: 

  p = subprocess.Popen(' | '.join(' '.join(words) for words in com), shell=True, ...)
  

 Alternatively, since you want stdout=None initially, you can use @pst's trick of simply having p initially be any object with a ..  But you should also note that you are depending on the system to close out each of your previous subprocess.Popen()s (so that their output pipes are close()d) inside the loop, which happens with CPython but not with Jython (as I understand itâ€”I have not actually used Jython).  So you might want a more explicit loop: 

  # assumes len(com) >= 1
p0 = subprocess.Popen(com[0], stdout=subprocess.PIPE)
# don't use stderr=subprocess.PIPE -- if you want stderr to be piped too
# use stderr=subprocess.STDOUT, or create a stderr pipe early manually and
# supply the fd directly; see hints in text below.
for words in com[1:]:
    p1 = subprocess.Popen(words, stdin=p0.stdout, stdout=subprocess.PIPE)
    # again you don't want to set stderr to subprocess.PIPE here
    p0.stdout.close() # now that it's going to p1, we must ditch ours
    p0 = p1 # p1 becomes the input to any new p1 we create next time
# at this point, use p0.communicate() to read output
  

 If you redirect stderr=subprocess.STDOUT in each Popen(), this will pipe the error output from each sub-command to the next one.  To pipe them all to your program, you will have to first create an OS-level pipe object, then connect each to that (single) pipe's write end, then sneak around the subprocess module to get the "final" subprocess object pointing to it, so that its select/poll code will suck any data out of that pipe.  (Since subprocess.communicate() will only read-multiple from two such pipes, you cannot capture each individual sub-command's stderr output independently.) 

 Note: none of the above is tested... 


-------------------------------------next answer-------------------------------------

Rank:10 (https://stackoverflow.com/questions/28572898)
 There are four strong cases for preferring Python's more-specific methods in the https://docs.python.org/2/library/os.html module over using https://docs.python.org/2/library/os.html#os.system or the https://docs.python.org/2/library/subprocess.html module when executing a command: 

 
  Redundancy  - spawning another process is redundant and wastes time and resources. 
  Portability  - Many of the methods in the  os  module are available in multiple platforms while many shell commands are os-specific. 
  Understanding the results  - Spawning a process to execute arbitrary commands forces you to parse the results from the output and understand  if  and  why  a command has done something wrong. 
  Safety  - A process can potentially execute any command it's given. This is a weak design and it can be avoided by using specific methods in the  os  module. 
 

 Redundancy (see http://en.wikipedia.org/wiki/Redundant_code): 

 You're actually executing a redundant "middle-man" on your way to the eventual system calls ( chmod  in your example). This middle man is a new process or sub-shell. 

 From https://docs.python.org/2/library/os.html#os.system: 

 
   Execute the command (a string) in a subshell ... 
 

 And https://docs.python.org/2/library/subprocess.html is just a module to spawn new processes. 

 You can do what you need without spawning these processes. 

 Portability (see http://en.wikipedia.org/wiki/Software_portability#Source_code_portability): 

 The https://docs.python.org/2/library/os.html module's aim is to provide generic operating-system services and it's description starts with: 

 
   This module provides a portable way of using operating system dependent functionality. 
 

 You can use https://docs.python.org/2/library/os.html#os.listdir on both windows and unix. Trying to use  os.system  /  subprocess  for this functionality will force you to maintain two calls (for  ls  /  dir ) and check what operating system you're on. This is not as portable and  will  cause even more frustration later on (see  Handling Output ). 

 Understanding the command's results: 

 Suppose you want to list the files in a directory. 

 If you're using  os.system("ls")  /  subprocess.call(['ls']) , you can only get the process's output back, which is basically a big string with the file names. 

 How can you tell a file with a space in it's name from two files? 

 What if you have no permission to list the files? 

 How should you map the data to python objects? 

 These are only off the top of my head, and while there are solutions to these problems - why solve again a problem that was solved for you? 

 This is an example of following the http://en.wikipedia.org/wiki/Don&#39;t_repeat_yourself principle (Often reffered to as "DRY") by  not  repeating an implementation that already exists and is freely available for you. 

 Safety: 

<p and  subprocess  are powerful. It's good when you need this power, but it's dangerous when you don't. When you use  os.listdir , you  know  it can not do anything else other then list files or raise an error. When you use  os.system  or  subprocess  to achieve the same behaviour you can potentially end up doing something you did not mean to do. 

  Injection Safety (see http://en.wikipedia.org/wiki/Code_injection#Shell_injection) : 

 If you use input from the user as a new command you've basically given him a shell. This is much like SQL injection providing a shell in the DB for the user. 

 An example would be a command of the form: 

  # ... read some user input
os.system(user_input + " some continutation")
  

 This can be easily exploited to run  any  arbitrary code using the input:  NASTY COMMAND;#  to create the eventual: 

  os.system("NASTY COMMAND; # some continuation")
  

 There are many such commands that can put your system at risk. 



